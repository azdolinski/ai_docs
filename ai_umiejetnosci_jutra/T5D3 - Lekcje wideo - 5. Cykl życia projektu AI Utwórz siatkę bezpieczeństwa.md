# Lekcje wideo - 5. Cykl Å¼ycia projektu AI UtwÃ³rz siatkÄ™ bezpieczeÅ„stwa

# ğŸ’¡ Diagram

```mermaid
mindmap
  root((ZarzÄ…dzanie Ryzykiem w AI))
    Ryzyko ZwiÄ…zane z AI
      Konsekwencje BÅ‚Ä™dÃ³w
        Straty Finansowe
        Utrata Reputacji
        ZagroÅ¼enie Å»ycia
      Potrzeba ZarzÄ…dzania Ryzykiem
    Siatka BezpieczeÅ„stwa
      Cykl Å»ycia Projektu AI
        Zdefiniowanie Problemu
        Gromadzenie Danych
        Alternatywy dla AI
        Stworzenie Siatki BezpieczeÅ„stwa
        WdroÅ¼enie Modelu
        Feedback
        Monitorowanie
      PrzykÅ‚ad Lekarza
        Decyzja CzÅ‚owieka
        OdpowiedzialnoÅ›Ä‡ Lekarza
        Kontrola Eksperta
        AI Nie Autonomiczne
    Poziomy Kontroli
      ZrÃ³Å¼nicowane Poziomy
      PrzykÅ‚ad Czat
        Minimalne Konsekwencje
        Ignorowanie Sugestii
    Macierz Ryzyka AI
      Osie Matrycy
        Stawka
          Wysoka
          Niska
        DokÅ‚adnoÅ›Ä‡
          Wysoka
          Niska
      Pola Matrycy
        Wysoka Stawka Niska DokÅ‚adnoÅ›Ä‡
          WERYFIKUJ Verify
          Zatwierdzenie CzÅ‚owieka
        Wysoka Stawka Wysoka DokÅ‚adnoÅ›Ä‡
          CO-PILOT CoPilot
          NadzÃ³r CzÅ‚owieka
        Niska Stawka Niska DokÅ‚adnoÅ›Ä‡
          IGNORUJ Disregard
          Decyzje Ludzi
        Niska Stawka Wysoka DokÅ‚adnoÅ›Ä‡
          AUTOMATYZUJ Automate
          PeÅ‚na Automatyzacja
```

___

# ğŸ—’ï¸ Notatka


# Notatki i Podsumowanie: ZarzÄ…dzanie Ryzykiem w Projektach AI

## Wprowadzenie

Niniejsze notatki dotyczÄ… **zarzÄ…dzania ryzykiem** w projektach **sztucznej inteligencji (AI)**. PodkreÅ›lono, Å¼e tworzenie modeli AI to nie tylko kwestia danych i algorytmÃ³w, ale przede wszystkim odpowiedzialnoÅ›Ä‡ za potencjalne konsekwencje ich dziaÅ‚ania w rzeczywistych warunkach. W materiale omÃ³wiono koncepcjÄ™ `siatki bezpieczeÅ„stwa` oraz **macierzy ryzyka AI**, ktÃ³re pomagajÄ… kontrolowaÄ‡ i minimalizowaÄ‡ ryzyko zwiÄ…zane z wdraÅ¼aniem systemÃ³w AI.

## Kluczowe Zagadnienia

### 1. Ryzyko ZwiÄ…zane z AI âš ï¸

* Tworzenie modeli AI to nie tylko kwestia danych i algorytmÃ³w, ale przede wszystkim **zarzÄ…dzanie ryzykiem**.
* AI, mimo poprawnego dziaÅ‚ania w Å›rodowisku testowym, moÅ¼e generowaÄ‡ **bÅ‚Ä™dy w rzeczywistych warunkach**, niosÄ…ce za sobÄ… konsekwencje, takie jak:
    * **Straty finansowe** ğŸ’°
    * **Utrata reputacji** ğŸ“‰
    * **ZagroÅ¼enie Å¼ycia ludzkiego** ğŸš‘ (w skrajnych przypadkach)
* Przed wdroÅ¼eniem modelu AI niezbÄ™dne jest stworzenie **`siatki bezpieczeÅ„stwa`** â€“ zespoÅ‚u mechanizmÃ³w kontroli i minimalizacji ryzyka bÅ‚Ä™dnych decyzji.

### 2. `Siatka BezpieczeÅ„stwa` w Cyklu Å»ycia Projektu AI ğŸ”„

* **Cykl Å¼ycia projektu AI** (schemat blokowy):
    1. **Zdefiniowanie problemu** ğŸ¯
    2. **Gromadzenie danych** ğŸ“Š
    3. **RozwaÅ¼enie alternatyw dla AI** (analiza rozwiÄ…zaÅ„ innych niÅ¼ AI) ğŸ¤”
    4. **Stworzenie `siatki bezpieczeÅ„stwa`** (kluczowy etap przed wdroÅ¼eniem) ğŸ›¡ï¸
    5. **WdroÅ¼enie modelu** ğŸš€
    6. **Zbieranie informacji zwrotnej** (feedback po wdroÅ¼eniu) ğŸ‘‚
    7. **Monitorowanie** (ciÄ…gÅ‚e Å›ledzenie dziaÅ‚ania modelu) ğŸ‘ï¸
* **PrzykÅ‚ad Lekarza i Diagnozy AI:** ğŸ§‘â€âš•ï¸
    * Lekarz korzysta z systemu AI, ktÃ³ry sugeruje diagnozy na podstawie milionÃ³w przypadkÃ³w.
    * **AI sugeruje, ale ostateczna decyzja naleÅ¼y do czÅ‚owieka (lekarza).**
    * Lekarz **ponosi odpowiedzialnoÅ›Ä‡** za potwierdzenie diagnozy, konsultacjÄ™ z pacjentem i decyzjÄ™ o leczeniu.
    * W kwestiach zdrowia i Å¼ycia **AI nie moÅ¼e dziaÅ‚aÄ‡ autonomicznie**.
    * **Kontrola eksperta (czÅ‚owieka) jest niezbÄ™dna.**

### 3. ZrÃ³Å¼nicowane Poziomy Kontroli w ZaleÅ¼noÅ›ci od Kontekstu âš–ï¸

* Nie kaÅ¼dy system AI wymaga tak **rygorystycznej kontroli**, jak w przypadku zastosowaÅ„ medycznych.
* **PrzykÅ‚ad Podpowiedzi w Czacie:** ğŸ’¬
    * AI proponuje odpowiedzi w oknie czatu, bazujÄ…c na kontekÅ›cie rozmowy.
    * BÅ‚Ä™dy AI w tym scenariuszu majÄ… **minimalne konsekwencje**.
    * UÅ¼ytkownik ma moÅ¼liwoÅ›Ä‡ **zignorowania sugestii** i wprowadzenia wÅ‚asnej wiadomoÅ›ci.

### 4. **Macierz Ryzyka AI** - Dostosowanie Nadzoru do Znaczenia Decyzji ğŸ“

* **Macierz ryzyka AI** (matryca) pomaga dopasowaÄ‡ poziom nadzoru do wagi decyzji podejmowanych przez AI.
* **Osie matrycy:**
    * **OÅ› pionowa: Stawka (Wysoka/Niska)** - konsekwencje bÅ‚Ä™dnej decyzji.
    * **OÅ› pozioma: DokÅ‚adnoÅ›Ä‡ (Niska/Wysoka)** - pewnoÅ›Ä‡ modelu AI.
* **Cztery pola matrycy:**
    * **Wysoka stawka, Niska dokÅ‚adnoÅ›Ä‡: WERYFIKUJ (Verify)** âœ…
        * AI sugeruje decyzjÄ™, **czÅ‚owiek musi jÄ… zatwierdziÄ‡**.
        * **PrzykÅ‚ad:** AI prognozuje reakcje pacjentÃ³w na leki, lekarz zatwierdza zmiany w terapii.
    * **Wysoka stawka, Wysoka dokÅ‚adnoÅ›Ä‡: CO-PILOT (CoPilot)** ğŸ§‘â€âœˆï¸
        * AI dziaÅ‚a bardziej autonomicznie, lecz nadal **pod nadzorem czÅ‚owieka**.
        * **PrzykÅ‚ad:** Systemy detekcji oszustw finansowych. AI identyfikuje i blokuje transakcje, analityk weryfikuje przed zamroÅ¼eniem konta.
    * **Niska stawka, Niska dokÅ‚adnoÅ›Ä‡: IGNORUJ (Disregard)** âŒ
        * **Ignoruj wyniki AI**, polegaj na decyzjach ludzi.
        * **PrzykÅ‚ad:** Systemy odpowiadajÄ…ce na komentarze w mediach spoÅ‚ecznoÅ›ciowych, ktÃ³re bÅ‚Ä™dnie interpretujÄ… ton wypowiedzi.
    * **Niska stawka, Wysoka dokÅ‚adnoÅ›Ä‡: AUTOMATYZUJ (Automate)** ğŸ¤–
        * **PeÅ‚na automatyzacja procesu.**
        * **PrzykÅ‚ad:** AI rekomendujÄ…ce utwory w serwisach streamingowych. BÅ‚Ä…d ma znikome konsekwencje (pominiÄ™cie utworu).

## Podsumowanie

WdraÅ¼anie modeli AI wiÄ…Å¼e siÄ™ z ryzykiem, ktÃ³rym naleÅ¼y aktywnie zarzÄ…dzaÄ‡. Kluczowym elementem jest **stworzenie `siatki bezpieczeÅ„stwa`**, zapewniajÄ…cej kontrolÄ™ nad dziaÅ‚aniem AI. Poziom nadzoru nad systemem AI powinien byÄ‡ **dostosowany do wagi podejmowanych decyzji oraz dokÅ‚adnoÅ›ci modelu**. **Macierz ryzyka AI** stanowi narzÄ™dzie pomocne w okreÅ›leniu adekwatnego poziomu kontroli â€“ od weryfikacji decyzji AI przez czÅ‚owieka, poprzez model dziaÅ‚ajÄ…cy jako co-pilot, aÅ¼ po peÅ‚nÄ… automatyzacjÄ™ w sytuacjach niskiego ryzyka. Zrozumienie i stosowanie zasad **zarzÄ…dzania ryzykiem** jest fundamentalne dla odpowiedzialnego i bezpiecznego wdraÅ¼ania technologii AI.


___

# ğŸ”‰ Transcript
File: Lekcje wideo - 5. Cykl Å¼ycia projektu AI UtwÃ³rz siatkÄ™ bezpieczeÅ„stwa.mp4<br>
[00:00:05] Budowanie modelu AI to nie tylko kwestia danych i algorytmÃ³w.
[00:00:10] To rÃ³wnieÅ¼ zarzÄ…dzanie ryzykiem.
[00:00:11] AI moÅ¼e dziaÅ‚aÄ‡ dobrze w warunkach testowych, ale co siÄ™ stanie, gdy zacznie podejmowaÄ‡ decyzjÄ™ w rzeczywistoÅ›ci?
[00:00:18] BÅ‚Ä™dy w AI mogÄ… kosztowaÄ‡, czasem pieniÄ…dze, czasem reputacjÄ™, a w niektÃ³rych przypadkach nawet ludzkie Å¼ycie.
[00:00:25] (Ekran: Schemat blokowy zatytuÅ‚owany "Cykl Å¼ycia projektu AI". Kolejne kroki to: 01. OkreÅ›l problem, 02. Zbieraj dane, 03. SprÃ³buj poza AI, 04. UtwÃ³rz siatkÄ™ bezpieczeÅ„stwa, 05. Wystawny model, 06. Zbieraj feedback, 07. Monitoruj.)
[00:00:25] Dlatego zanim wdroÅ¼ymy model, musimy stworzyÄ‡ tak zwanÄ… siatkÄ™ bezpieczeÅ„stwa, ktÃ³ra pozwoli nam kontrolowaÄ‡ jego dziaÅ‚anie i zminimalizowaÄ‡ ryzyko bÅ‚Ä™dnych decyzji.
[00:00:35] (Ekran: ZdjÄ™cie lekarza z tabletem rozmawiajÄ…cego z pacjentkÄ….)
[00:00:35] WyobraÅºcie sobie lekarza, ktÃ³ry przekazuje pacjentowi diagnozÄ™, ale zamiast samemu analizowaÄ‡ wyniki badaÅ„, korzysta z systemu AI, ktÃ³ry sugeruje moÅ¼liwe schorzenia.
[00:00:45] Model zostaÅ‚ wytrenowany na milionach przypadkÃ³w, a jego skutecznoÅ›Ä‡ jest wysoka.
[00:00:50] Czy oznacza to, Å¼e lekarz moÅ¼e bezrefleksyjnie przekazaÄ‡ pacjentowi wynik od AI?
[00:00:55] OczywiÅ›cie, Å¼e nie.
[00:00:56] To wÅ‚aÅ›nie przykÅ‚ad siatki bezpieczeÅ„stwa, w ktÃ³rej czÅ‚owiek pozostaje ostatecznym decydentem.
[01:02] AI moÅ¼e sugerowaÄ‡ diagnozÄ™, ale to lekarz bierze odpowiedzialnoÅ›Ä‡ za jej potwierdzenie, konsultuje siÄ™ z pacjentem i podejmuje decyzjÄ™ o leczeniu.
[01:10] W sytuacjach, gdzie stawkÄ… jest ludzkie zdrowie i Å¼ycie, AI nie moÅ¼e dziaÅ‚aÄ‡ autonomicznie.
[01:14] Potrzebna jest kontrola eksperta.
[01:17] (Ekran: Dwa ekrany smartfonÃ³w z konwersacjami tekstowymi.)
[01:18] Nie kaÅ¼dy system AI wymaga tak Å›cisÅ‚ej kontroli.
[01:21] WyobraÅºmy sobie inny przypadek, automatyczne podpowiedzi w czacie.
[01:25] AI analizuje kontekst rozmowy i sugeruje uÅ¼ytkownikowi gotowe odpowiedzi.
[01:30] Nawet jeÅ›li czasem system siÄ™ pomyli i nie zrozumie intencji, konsekwencje sÄ… niewielkie.
[01:35] UÅ¼ytkownik po prostu zignoruje sugestiÄ™ i wpisze wÅ‚asnÄ… wiadomoÅ›Ä‡.
[01:40] (Ekran: Macierz ryzyka AI. OÅ› pionowa: High stakes, Low stakes. OÅ› pozioma: Low accuracy, High accuracy. Cztery pola: Verify, CoPilot, Disregard, Automate.)
[01:40] Dlatego tak waÅ¼ne jest dopasowanie poziomu nadzoru do wagi decyzji, jakie podejmuje AI.
[01:46] Wykorzystujemy do tego matrycÄ™ ryzyka AI, ktÃ³ra pomaga okreÅ›liÄ‡, kiedy model moÅ¼e dziaÅ‚aÄ‡ samodzielnie, a kiedy konieczna jest interwencja czÅ‚owieka.
[01:55] JeÅ›li mamy wysokÄ… stawkÄ™ i niskÄ… dokÅ‚adnoÅ›Ä‡, model powinien jedynie sugerowaÄ‡ decyzjÄ™, a czÅ‚owiek powinien je zatwierdzaÄ‡.
[02:02] PrzykÅ‚ad?
[02:03] AI przewiduje, ktÃ³rzy pacjenci mogÄ… Åºle zareagowaÄ‡ na lek, ale lekarz musi potwierdziÄ‡ kaÅ¼dÄ… zmianÄ™ w terapii.
[02:09] Gdy stawka jest wysoka, ale model jest bardzo dokÅ‚adny, moÅ¼emy pozwoliÄ‡ mu dziaÅ‚aÄ‡ nieco bardziej autonomicznie, ale nadal pod okiem czÅ‚owieka.
[02:17] PrzykÅ‚ad?
[02:18] Systemy wykrywajÄ…ce oszustwa finansowe w bankach.
[02:21] AI moÅ¼e wskazywaÄ‡ i blokowaÄ‡ podejrzane transakcje, ale to analityk nadal weryfikuje je przed zamroÅ¼eniem konta.
[02:27] Z kolei, gdy stawka jest niska i dokÅ‚adnoÅ›Ä‡ modelu niewielka, najlepiej caÅ‚kowicie zignorowaÄ‡ wyniki AI i polegaÄ‡ na ludziach.
[02:34] Tak dzieje siÄ™ na przykÅ‚ad w przypadku systemÃ³w, ktÃ³re prÃ³bujÄ… odpowiadaÄ‡ na komentarze w social media, ale czÄ™sto bÅ‚Ä™dnie interpretujÄ… ton wypowiedzi.
[02:42] JeÅ›li jednak stawka jest niska, a dokÅ‚adnoÅ›Ä‡ wysoka, moÅ¼emy w peÅ‚ni zautomatyzowaÄ‡ proces.
[02:47] PrzykÅ‚ad?
[02:48] AI, ktÃ³re rekomendujÄ… utwory w serwisach streamingowych.
[02:51] Nawet jeÅ›li model siÄ™ pomyli, uÅ¼ytkownik po prostu pominie dany utwÃ³r.
[02:56] [background music ends]

___
# ğŸ·ï¸ Tags
#zarzÄ…dzanie_ryzykiem #ryzyko #AI #sztuczna_inteligencja #model_AI #bÅ‚Ä™dy_AI #straty_finansowe #utrata_reputacji #zagroÅ¼enie_Å¼ycia #siatka_bezpieczeÅ„stwa #cykl_Å¼ycia_projektu_AI #definiowanie_problemu #gromadzenie_danych #alternatywy_dla_AI #wdroÅ¼enie_modelu #informacja_zwrotna #monitorowanie #lekarz #diagnoza_AI #odpowiedzialnoÅ›Ä‡ #kontrola_eksperta #czat #podpowiedzi_w_czacie #minimalne_konsekwencje #macierz_ryzyka_AI #stawka #dokÅ‚adnoÅ›Ä‡ #WERYFIKUJ #Verify #CO-PILOT #CoPilot #IGNORUJ #Disregard #AUTOMATYZUJ #Automate #nadzÃ³r #automatyzacja #bezpieczeÅ„stwo #rygorystyczna_kontrola #kontekst #decyzje #oszustwa_finansowe #rekomendacje #streaming #systemy_rekomendacyjne #modele_uczenia_maszynowego #machine_learning
